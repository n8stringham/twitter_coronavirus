# Coronavirus twitter analysis

This repo contains an analysis of geotagged tweets sent throughout 2020.
Specifically, we count the number of tweets sent with various hashtags related to the coronavirus
and explore how these tweets are distributed across different languages and countries.
In order to speed up our analysis we use [MapReduce](https://en.wikipedia.org/wiki/MapReduce) to parallelize the computation of these count statistics.

## The Data

Approximately 500 million tweets are sent everyday.
Of those tweets, about 1% are *geotagged*.
That is, the user's device includes location information about where the tweets were sent from.
In total, there are about 1.1 billion tweets in this dataset (693G disk space)


**Runtime:**

The simplest and most common scenario is that the map procedure takes time O(n) and the reduce procedure takes time O(1).
If you have p<<n processors, then the overall runtime will be O(n/p).
This means that:
1. doubling the amount of data will cause the analysis to take twice as long;
1. doubling the number of processors will cause the analysis to take half as long;
1. if you want to add more data and keep the processing time the same, then you need to add a proportional number of processors.


## Analysis 

MapReduce is a famous procedure for large scale parallel processing that is widely used in industry.
It is a 3 step procedure summarized in the following image:

<img src=mapreduce.png width=100% />


1. **Partitioning:**
    This step organizes the data in a way that allows for subsets of the data to be processed independently from each other.

    The tweets are stored as follows:
    The tweets for each day are stored in a zip file `geoTwitterYY-MM-DD.zip`,
    and inside this zip file are 24 text files, one for each hour of the day.
    Each text file contains a single tweet per line in JSON format.
    
   
2. **Mapping:**
   The `map.py` file processes a single zip file of tweets.
   It gets the counts for each hashtag grouped by language and country.

   This summary information is formatted in JSON and stored in a new file.
   

3. **Reducing:**
   The `reduce.py` file merges the outputs generated by the `map.py` file so that the counts can be aggregated and visualized.

   ```
   $ ./src/reduce.py --input_paths outputs/geoTwitter*.lang --output_path=reduced.lang
   ```
   will create a file containing the aggregated counts grouped by language for each hashtag while

   ```
   $ ./src/reduce.py --input_paths outputs/geoTwitter*.country --output_path=reduced.country
   ```
   will create a file where the counts are grouped by country.


## Results

After completing the map reduce process we are able to look at the frequency with which different hashtags were used in different countries/languages during 2020.

For example if we want to know how many tweets used `#coronavirus` in each language, we can use the command 
   ```
   $ ./src/visualize.py --input_path=reduced.lang --key='#coronavirus'

   OUTPUT
   en : 422394
   es : 137714
   und : 103249
   it : 29812
   pt : 28783
   fr : 26349
   tr : 18167
   hi : 15943
   de : 12156
   nl : 9089
   ```
similarly we can get the breakdown by country

   ```
   $ ./src/visualize.py --input_path=reduced.lang --key='#coronavirus'

   OUTPUT
   US : 225402
   IN : 89063
   GB : 66803
   ES : 36080
   IT : 35668
   BR : 31637
   AR : 30992
   MX : 24146
   FR : 22562
   TR : 22172
   ```

We also track some hashtags in languages other than English.
For example, `#coronavirus` translates to `#冠状病毒` in Chinese.
Our analysis shows this hashtag was used according the following geographic distribution.

    ```
    YE : 45
    CN : 25
    US : 23
    JP : 7
    SG : 3
    ES : 3
    TW : 2
    PH : 2
    IN : 2
    AE : 2
    ```
Interestingly, this hashtag was used most by users in Yemen, followed by China and then the US.


The vizualization output for each hashtag can be found in the `viz` folder of this repo.
These can be used to uncover more insights about the geographic and linguistic distribution of coronavirus-related tweets during 2020.
